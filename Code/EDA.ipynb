{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "with open('../Data/data.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    print(result['encoding'])\n",
    "\n",
    "encoding = 'Windows-1252'  \n",
    "\n",
    "# Read the CSV file with the detected encoding\n",
    "raw_df = pd.read_csv('../Data/data.csv', encoding=encoding, header=0)  \n",
    "\n",
    "# Set column name as 'sentiment' and 'raw_comm'\n",
    "raw_df.columns = ['sentiment', 'raw_comm']\n",
    "\n",
    "print(raw_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a comment column with only nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Combine all comments into a single string\n",
    "text = \" \".join(raw_df['raw_comm'])\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to extract nouns from a comment\n",
    "def extract_nouns(text):\n",
    "    doc = nlp(text)\n",
    "    nouns = [token.text for token in doc if token.pos_ == 'NOUN']\n",
    "    return \" \".join(nouns)\n",
    "\n",
    "# Apply the function to each comment\n",
    "raw_df['nouns_comm'] = raw_df['raw_comm'].apply(extract_nouns)\n",
    "\n",
    "# Display the DataFrame with extracted nouns\n",
    "print(raw_df.head())\n",
    "\n",
    "# Save this new df to a new CSV file\n",
    "raw_df.to_csv('Data/clean_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment count distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chardet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('Data/clean_data.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    print(result['encoding'])\n",
    "\n",
    "encoding = 'Windows-1252'  \n",
    "\n",
    "# Read the CSV file with the detected encoding\n",
    "df = pd.read_csv('Data/clean_data.csv', encoding=encoding, header=0) \n",
    "\n",
    "# Plot the sentiment distribution\n",
    "sentiment_count = df['sentiment'].value_counts()\n",
    "sentiment_count.plot(kind = 'bar')\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud for all sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom stopwords\n",
    "custom_stopwords = set(STOPWORDS)\n",
    "custom_stopwords.update(['will', 'mn', 'm']) \n",
    "\n",
    "# Create a word cloud object\n",
    "wordcloud = WordCloud(\n",
    "    width=800,                # Width of the canvas\n",
    "    height=400,               # Height of the canvas\n",
    "    background_color='white', # Background color of the word cloud\n",
    "    max_words=50,            # Maximum number of words to display\n",
    "    contour_color='steelblue',# Color of the contour\n",
    "    contour_width=1,          # Width of the contour line\n",
    "    colormap='viridis',       # Color map for the words\n",
    "    stopwords=custom_stopwords            # Stopwords to exclude (default is None)\n",
    ").generate(df['nouns_comm'].str.cat(sep=' '))  # Concatenate all nouns into a single string\n",
    "\n",
    "# Display the generated word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Turn off axis lines and labels\n",
    "plt.title('Word Cloud of Comments', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordclourd per sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "def plot_wordcloud(text, title):\n",
    "    # Custom stopwords\n",
    "    custom_stopwords = set(STOPWORDS)\n",
    "    custom_stopwords.update(['will', 'mn', 'mln,' 'Company', 'Finnish', 'year']) \n",
    "\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=75, stopwords=custom_stopwords).generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# Fill NaN values in the 'nouns' column with an empty string\n",
    "df['nouns_comm'] = df['nouns_comm'].fillna('')\n",
    "\n",
    "positive_text = \" \".join(df[df['sentiment'] == 'positive']['nouns_comm'])\n",
    "negative_text = \" \".join(df[df['sentiment'] == 'negative']['nouns_comm'])\n",
    "neutral_text = \" \".join(df[df['sentiment'] == 'neutral']['nouns_comm'])\n",
    "\n",
    "plot_wordcloud(positive_text, 'Positive Comments')\n",
    "plot_wordcloud(negative_text, 'Negative Comments')\n",
    "plot_wordcloud(neutral_text, 'Neutral Comments')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text length distribution per sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment_length'] = df['raw_comm'].apply(len)\n",
    "\n",
    "df.hist(column='comment_length', by='sentiment', bins=20, figsize=(10, 6))\n",
    "plt.suptitle('Distribution of Comment Lengths by Sentiment')\n",
    "plt.show()\n",
    "\n",
    "# Comment length mean by sentiment\n",
    "mean_length = df.groupby('sentiment')['comment_length'].mean()\n",
    "\n",
    "mean_length.plot(kind='bar')\n",
    "plt.title('Mean Comment Length by Sentiment')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Mean Comment Length')\n",
    "plt.show()\n",
    "\n",
    "print(mean_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fill NaN values in the 'nouns' column with an empty string\n",
    "df['nouns_comm'] = df['nouns_comm'].fillna('')\n",
    "\n",
    "# Calculate the length of the noun phrases\n",
    "df['nouns_length'] = df['nouns_comm'].apply(len)\n",
    "\n",
    "# Set the style of the plot\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# Filter the noun lengths less than or equal to 250\n",
    "filtered_df = df[df['nouns_length'] <= 250]\n",
    "\n",
    "# Plot the distribution of noun lengths for each sentiment\n",
    "sns.histplot(data=filtered_df, x=\"nouns_length\", hue=\"sentiment\", multiple=\"stack\", kde=True)\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title(\"Distribution of Noun Lengths for each Sentiment\")\n",
    "plt.xlabel(\"Noun Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Add a label for noun lengths greater than 250\n",
    "plt.text(250, 250, '250+', ha='center', va='center', color='white', fontweight='bold', fontsize=12,\n",
    "         bbox=dict(facecolor='black', edgecolor='black', boxstyle='round,pad=0.5'))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common words per sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def get_top_n_words(corpus, n=10, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = set()\n",
    "    words = re.findall(r'\\w+', corpus.lower())\n",
    "    filtered_words = [word for word in words if word not in stopwords]\n",
    "    return Counter(filtered_words).most_common(n)\n",
    "\n",
    "# Define custom stopwords\n",
    "custom_stopwords = {'mn', 'mln', 'will','m', 's', 'eur', 'pct', 'company', 'said', 'finnish', 'year'}\n",
    "\n",
    "# Get top words while excluding stopwords\n",
    "positive_words = get_top_n_words(positive_text, 10, stopwords=custom_stopwords)\n",
    "negative_words = get_top_n_words(negative_text, 10, stopwords=custom_stopwords)\n",
    "neutral_words = get_top_n_words(neutral_text, 10, stopwords=custom_stopwords)\n",
    "\n",
    "print('Top 10 words in positive comments:', positive_words)\n",
    "print('Top 10 words in negative comments:', negative_words)\n",
    "print('Top 10 words in neutral comments:', neutral_words)\n",
    "\n",
    "# Plotting the top words for each sentiment as subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot for positive sentiment\n",
    "positive_words_dict = dict(positive_words)\n",
    "axs[0].bar(positive_words_dict.keys(), positive_words_dict.values(), color='green')\n",
    "axs[0].set_title('Top 10 Words in Positive Comments')\n",
    "axs[0].set_xlabel('Words')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "axs[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot for negative sentiment\n",
    "negative_words_dict = dict(negative_words)\n",
    "axs[1].bar(negative_words_dict.keys(), negative_words_dict.values(), color='red')\n",
    "axs[1].set_title('Top 10 Words in Negative Comments')\n",
    "axs[1].set_xlabel('Words')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "axs[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot for neutral sentiment\n",
    "neutral_words_dict = dict(neutral_words)\n",
    "axs[2].bar(neutral_words_dict.keys(), neutral_words_dict.values(), color='blue')\n",
    "axs[2].set_title('Top 10 Words in Neutral Comments')\n",
    "axs[2].set_xlabel('Words')\n",
    "axs[2].set_ylabel('Frequency')\n",
    "axs[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Function to get top n-grams\n",
    "def get_top_n_ngrams(corpus, n=10, ngram_range=(2, 2), stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "    \n",
    "    # Convert stopwords to a list if it's a set\n",
    "    stopwords = list(stopwords)\n",
    "    \n",
    "    vec = CountVectorizer(ngram_range=ngram_range, stop_words=stopwords).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return words_freq[:n]\n",
    "\n",
    "# Define custom stopwords\n",
    "custom_stopwords = ['mn', 'mln', 'will', 'm', 's', 'eur', 'pct', 'company', 'said', 'finnish', 'year']\n",
    "\n",
    "# Get top bigrams while excluding custom stopwords\n",
    "positive_bigrams = get_top_n_ngrams(df[df['sentiment'] == 'positive']['nouns_comm'], 10, (2, 2), stopwords=custom_stopwords)\n",
    "negative_bigrams = get_top_n_ngrams(df[df['sentiment'] == 'negative']['nouns_comm'], 10, (2, 2), stopwords=custom_stopwords)\n",
    "neutral_bigrams = get_top_n_ngrams(df[df['sentiment'] == 'neutral']['nouns_comm'], 10, (2, 2), stopwords=custom_stopwords)\n",
    "\n",
    "# Print the results\n",
    "print('Top 10 bigrams in positive comments:', positive_bigrams)\n",
    "print('Top 10 bigrams in negative comments:', negative_bigrams)\n",
    "print('Top 10 bigrams in neutral comments:', neutral_bigrams)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
